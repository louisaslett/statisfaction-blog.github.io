<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Statisfaction - I can&#39;t get no</title>
<link>https://github.com/statisfaction-blog/index.html</link>
<atom:link href="https://github.com/statisfaction-blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Tue, 31 Oct 2023 23:00:00 GMT</lastBuildDate>
<item>
  <title>Quantum workers in Bernoulli factories</title>
  <dc:creator>Rémi Bardenet</dc:creator>
  <link>https://github.com/statisfaction-blog/posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html</link>
  <description><![CDATA[ 




<p>TL;DR: A quantum computer lets you provably build more general Bernoulli factories than your laptop.</p>
<p>I have grown an interest for quantum computing, both for fun and because it naturally applies to sampling my favourite distribution, <a href="https://arxiv.org/abs/2305.15851">determinantal point processes</a>. One of the natural big questions in quantum computing is, for a given computational task such as solving a linear system, whether having access to a quantum computer gives you an <em>advantage</em> over using your laptop in the smartest way possible. The advantage can be with respect to any resource: maybe the quantum computer lets you solve your problem faster, or maybe it allows you to solve a more general class of problems. <a href="https://www.nature.com/articles/ncomms9203">Dale, Jennings, and Rudolph (2015)</a> prove a quantum advantage of the latter kind, for a task that appeals to a computational statistician: a quantum computer gives you access to strictly more Bernoulli factories than your laptop does. In this post, I discuss one of their examples.</p>
<p><em>Bernoulli factories.</em> First, I need to define what a Bernoulli factory is. Loosely speaking, a Bernoulli factory is an algorithm that, when fed with i.i.d draws from a Bernoulli random variable <img src="https://latex.codecogs.com/png.latex?B(p)"> with unknown parameter <img src="https://latex.codecogs.com/png.latex?p">, outputs a stream of independent Bernoullis with parameter <img src="https://latex.codecogs.com/png.latex?f(p)">. The algorithm does not have access to the value of <img src="https://latex.codecogs.com/png.latex?p">, and needs to work for as large a range of values of <img src="https://latex.codecogs.com/png.latex?p"> as possible. For instance, a trick attributed to von Neumann gives you a Bernoulli factory for the constant function <img src="https://latex.codecogs.com/png.latex?f%5Cequiv%201/2">, can you guess how? The problem of determining what Bernoulli factories can be constructed on a <em>classical</em> (as opposed to <em>quantum</em>) computer has been answered by <a href="https://dl.acm.org/doi/10.1145/175007.175019">Keane and O’Brien (1994)</a>. Essentially, it is necessary and sufficient that <img src="https://latex.codecogs.com/png.latex?(i)"> <img src="https://latex.codecogs.com/png.latex?f"> be continuous on its domain <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">, and that <img src="https://latex.codecogs.com/png.latex?(ii)"> either <img src="https://latex.codecogs.com/png.latex?f"> is constant or there exists an integer <img src="https://latex.codecogs.com/png.latex?n"> such that, for all <img src="https://latex.codecogs.com/png.latex?p%5Cin%5Cmathcal%7BP%7D">, <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmin%5B%20f(p),%201-f(p)%5D%20%5Cgeq%20%5Cmin%20%5B%20p%5En,%20(1-p)%5En%20%5D.%0A"> In particular, a non-constant <img src="https://latex.codecogs.com/png.latex?f"> should not take the values <img src="https://latex.codecogs.com/png.latex?0"> or <img src="https://latex.codecogs.com/png.latex?1"> in <img src="https://latex.codecogs.com/png.latex?(0,1)">, and cannot approach these extreme values too fast. In particular, the doubling function <img src="https://latex.codecogs.com/png.latex?f_%5Cmathrm%7Bdouble%7D:p%5Cmapsto%202p"> defined on <img src="https://latex.codecogs.com/png.latex?%5B0,1/2%5D"> does not correspond to a Bernoulli factory, while its restriction to <img src="https://latex.codecogs.com/png.latex?%5B0,1/2-%5Cepsilon%5D"> does, for any <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0">. Another simple example is <img src="https://latex.codecogs.com/png.latex?f_%5Cmathrm%7Bquadratic%7D:p%5Cmapsto%204p(1-p)"> defined on <img src="https://latex.codecogs.com/png.latex?%5B0,1%5D">, which does not correspond to a Bernoulli factory. We now show that <img src="https://latex.codecogs.com/png.latex?h"> can be realized simply on a quantum computer.</p>
<p><em>Quantum computers.</em> Now buckle up, because I need to define a quantum computer in a few words. Let <img src="https://latex.codecogs.com/png.latex?N"> be a positive integer, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%20=%20(%5Cmathbb%7BC%7D%5E2)%5E%7B%5Cotimes%20N%7D%20=%20%5Cmathbb%7BC%7D%5E2%5Cotimes%20%5Cdots%20%5Cotimes%20%5Cmathbb%7BC%7D%5E2">, where the tensor product is taken <img src="https://latex.codecogs.com/png.latex?N"> times. An <img src="https://latex.codecogs.com/png.latex?N">-qubit quantum computer is a machine that, when fed with</p>
<ol type="1">
<li>a linear operator <img src="https://latex.codecogs.com/png.latex?%5Crho"> on the Hilbert space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D">, with trace norm <img src="https://latex.codecogs.com/png.latex?1"> (the <em>state</em>),</li>
<li>a Hermitian operator <img src="https://latex.codecogs.com/png.latex?A"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D%5E2%5Cotimes%20%5Cmathbb%7BC%7D%5E2"> (the <em>observable</em>),</li>
</ol>
<p>outputs a draw from the random variable <img src="https://latex.codecogs.com/png.latex?X">, with support the spectrum of <img src="https://latex.codecogs.com/png.latex?A">, defined by <span id="eq-born"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathbb%7BE%7D%20g(X)%20=%20%5Cmathrm%7BTr%7D(%5Crho%20g(A)),%20%5Cquad%20g:%5Cmathbb%7BH%7D%5Crightarrow%20%5Cmathbb%7BR%7D_+.%0A%5Ctag%7B1%7D"></span> Here <img src="https://latex.codecogs.com/png.latex?g(A)"> is the operator that has the same eigenvectors as <img src="https://latex.codecogs.com/png.latex?A">, but where each eigenvalue <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is replaced by <img src="https://latex.codecogs.com/png.latex?g(%5Clambda)">. As an example, consider an eigenpair <img src="https://latex.codecogs.com/png.latex?(%5Clambda,%20v)"> of <img src="https://latex.codecogs.com/png.latex?A">, and take for state the orthogonal projector <img src="https://latex.codecogs.com/png.latex?%5Crho%20=%20%5CPi_v"> onto <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7Dv">. Then Equation&nbsp;1 $$ (X_{A,}=) = (A) = (_v A) = $.<br>
Finally, it is assumed that repeated calls to the computer output independent draws.</p>
<p><em>Quantum Bernoulli factories.</em> We first need some equivalent of a Bernoulli random variable. Consider an orthonormal basis <img src="https://latex.codecogs.com/png.latex?(e_0,%20e_1)"> of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D%5E2">. A natural candidate is the projector <img src="https://latex.codecogs.com/png.latex?%5CPi_p"> in $^2 onto <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Csqrt%7B1-p%7De_0%20+%20%5Csqrt%7Bp%7De_1%0A"> Indeed, <img src="https://latex.codecogs.com/png.latex?%5CPi_p"> together with the observable <img src="https://latex.codecogs.com/png.latex?%5CPi_0%20+%20%5CPi_1"> onto <img src="https://latex.codecogs.com/png.latex?e_0"> as observable define through Equation&nbsp;1 a binary random variable <img src="https://latex.codecogs.com/png.latex?X"> such that <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Cmathbb%20P%20(X=1)%20=%20%5Cmathrm%7BTr%7D%5B%20%5CPi_p%20%5CPi_1%20%5D%20=%20p%20=%201%20-%20%5Cmathbb%20P%20(X=0).%0A"></p>
<p>A quantum Bernoulli factory will be defined as an algorithm that takes <img src="https://latex.codecogs.com/png.latex?f"> and its domain as input, and outputs a number of qubits <img src="https://latex.codecogs.com/png.latex?N">, a state-observable pair <img src="https://latex.codecogs.com/png.latex?(%5Crho,%20A)"> on <img src="https://latex.codecogs.com/png.latex?(%5Cmathbb%7BC%7D%5E2)%5E%7B%5Cotiems%20N%7D">, and a function <img src="https://latex.codecogs.com/png.latex?h"> such that <img src="https://latex.codecogs.com/png.latex?h(X_%7BA,%5Crho%7D)%5Csim%20%5Cmathrm%7BBer%7D(f(p))">.</p>
<p>so that <img src="https://latex.codecogs.com/png.latex?(e_i%5Cotimes%20e_j)_%7B0%5Cleq%20i,j,%20%5Cleq%201%7D"> gives a basis of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D">. Actually, I will need to independent Bernoulli draws to realize <img src="https://latex.codecogs.com/png.latex?f_%5Cmathrm%7Bquadratic%7D">, so I directly define the equivalent of two independent Bernoulli draws. This is the state <img src="https://latex.codecogs.com/png.latex?v_p%5Cin%5Cmathbb%7BC%5E2%7D"> defined by $$</p>
<p>$$ It does represent our pair of Bernoulli draws in the following sense. If I feed my computer if you take your state <img src="https://latex.codecogs.com/png.latex?v_p"> and the observable</p>



 ]]></description>
  <category>quantum computing</category>
  <category>simulation</category>
  <guid>https://github.com/statisfaction-blog/posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html</guid>
  <pubDate>Tue, 31 Oct 2023 23:00:00 GMT</pubDate>
</item>
<item>
  <title>Coulomb rhymes with variance reduction</title>
  <dc:creator>Rémi Bardenet</dc:creator>
  <link>https://github.com/statisfaction-blog/posts/01-11-2023-repelled-point-processes/repelled_point_processes.html</link>
  <description><![CDATA[ 




<p>… Well, it does rhyme if you read the title aloud with a French accent, hon hon hon.</p>
<p>To paraphrase Nicolas’s previous <a href="https://statisfaction.wordpress.com/2022/12/22/how-to-beat-monte-carlo-no-qmc/">post</a>, say I want to approximate the integral <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20I(f)%20:=%20%5Cint_%7BS%7D%20f(u)%20du,%0A"> where <img src="https://latex.codecogs.com/png.latex?S"> is a compact set of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed">. I could use plain old Monte Carlo with <img src="https://latex.codecogs.com/png.latex?N"> nodes, <span id="eq-mc"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Chat%7BI%7D(f)%20=%20%5Cfrac%201%20N%20%5Csum_%7Bi=1%7D%5EN%20f(U_i),%20%20%20%5Cquad%20U_i%20%5Csim%20%5Cmathrm%7BU%7D(S).%0A%5Ctag%7B1%7D"></span> Intuitively, an i.i.d. uniform sample of quadrature nodes <img src="https://latex.codecogs.com/png.latex?U_1,%20%5Cdots,%20U_N"> will however leave “holes”; see Figure&nbsp;1 (a). In words, given a realization of the nodes, it is possible to insert a few large balls in <img src="https://latex.codecogs.com/png.latex?S"> that do not contain any <img src="https://latex.codecogs.com/png.latex?U_i">. These holes may make us miss some large variations of <img src="https://latex.codecogs.com/png.latex?f">. Part of the variance of the Monte Carlo estimator in Equation&nbsp;1 could intuitively be removed if we managed to fill these holes, using some of the nodes that got lumped together by chance.</p>
<p>Many sampling algorithms, such as randomized quasi-Monte Carlo, impose similar space-filling constraints, yielding a random sample with guarantees of “well-spreadedness”. In the <a href="https://arxiv.org/abs/2308.04825">paper</a> I describe in this post, <a href="https://dhawat.github.io/">Diala Hawat</a> and her two advisors (Raphaël Lachièze-Rey and myself) obtained variance reduction by explicitly trying to fill the holes left by a realization of <img src="https://latex.codecogs.com/png.latex?U_1,%20%5Cdots,%20U_N">. In the remainder of the post, I will describe Diala’s main theoretical result.</p>
<div id="fig-samples" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-poisson" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><embed src="poisson.pdf" class="img-fluid" data-ref-parent="fig-samples"></p>
<figcaption class="figure-caption">(a) A Poisson sample</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-repelled" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><embed src="repelled.pdf" class="img-fluid" data-ref-parent="fig-samples"></p>
<figcaption class="figure-caption">(b) The same sample after repulsion</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Note how the repelled sample has fewer visible “holes” and “lumps”. The details of how we implemented the repulsion are interesting in themselves, and can be found in the <a href="https://arxiv.org/abs/2308.04825">paper</a> and the associated <a href="https://github.com/dhawat/MCRPPy">code</a>.</figcaption><p></p>
</figure>
</div>
<p>The basic intuition is to imagine the quadrature nodes <img src="https://latex.codecogs.com/png.latex?U_1,%20%5Cdots,%20U_N"> as electrons. In physics, electrons (like all charged particles) are subject to the <a href="https://en.wikipedia.org/wiki/Coulomb%27s_law">Coulomb force</a>. The Coulomb force exerted by one electron onto another points away from the first electron, with a magnitude that is inversely proportional to the <img src="https://latex.codecogs.com/png.latex?d-1">th power of the Euclidean distance between the two. As a result, electrons tend to repel each other, and electrons close to you will push you away harder than electrons at the other side of the support of <img src="https://latex.codecogs.com/png.latex?f">. This is the behaviour that we would like to emulate, so that our quadrature nodes avoid lumping together and rather go and fill holes where no particle causes any repulsion.</p>
<p>If we solved the differential equation implementing Coulomb’s repulsion on our <img src="https://latex.codecogs.com/png.latex?N"> i.i.d. nodes, however, the points would rapidly leave the support of <img src="https://latex.codecogs.com/png.latex?f"> and “go to infinity”, to make sure that the pairwise distances between nodes are as large as possible. One way to avoid this undesired behaviour is to consider an “infinite” uniform Monte Carlo sample in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed">, so that, wherever an electron looks, there are an infinite number of electrons preventing it from escaping. To make the situation comparable with our initial <img src="https://latex.codecogs.com/png.latex?N">-point estimator in Equation&nbsp;1, we also require that there are roughly <img src="https://latex.codecogs.com/png.latex?N"> points inside the region <img src="https://latex.codecogs.com/png.latex?S"> where we integrate <img src="https://latex.codecogs.com/png.latex?f">. Formally, we consider a homogeneous Poisson point process <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D"> of intensity <img src="https://latex.codecogs.com/png.latex?%5Crho%20=%20N/V"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed">, where <img src="https://latex.codecogs.com/png.latex?V"> is the volume of <img src="https://latex.codecogs.com/png.latex?S">. Consider the modified Monte Carlo estimator <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Ctilde%7BI%7D(f)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bx%5Cin%20S%5Ccap%5Cmathcal%7BP%7D%7D%20f(x).%0A"> This estimator is very similar to the <img src="https://latex.codecogs.com/png.latex?N">-point crude Monte Carlo estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7BI%7D(f)">, except the number of evaluations of <img src="https://latex.codecogs.com/png.latex?f"> in the sum is now Poisson-distributed, with mean and variance <img src="https://latex.codecogs.com/png.latex?N">. What we have gained is that we can now intuitively apply the Coulomb force to the points of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">, and hope that both before and after repulsion, about <img src="https://latex.codecogs.com/png.latex?N"> points remain in our integration domain <img src="https://latex.codecogs.com/png.latex?S">. Proving this remains technically thorny, however. For starters, for <img src="https://latex.codecogs.com/png.latex?x"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed">, the series defining the Coulomb force exerted on <img src="https://latex.codecogs.com/png.latex?x"> by a collection <img src="https://latex.codecogs.com/png.latex?C"> of points in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ed">, namely <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20F_C(x)%20=%20%5Csum_%7By%5Cin%20C,%20y%5Cneq%20x%7D%20%5Cfrac%7Bx-y%7D%7B%5CVert%20x-y%5CVert%5E%7Bd%7D%7D,%0A"> is not absolutely convergent, so that the order of summation matters. However, it was observed as early as 1943 that, if you sum by increasing distance to the reference point <img src="https://latex.codecogs.com/png.latex?x">, and <img src="https://latex.codecogs.com/png.latex?C=%5Cmathcal%7BP%7D"> is a homogeneous Poisson point process, then the (random) series <img src="https://latex.codecogs.com/png.latex?F_%5Cmathcal%7BP%7D(x)"> converges almost surely. Interested readers are referred to a classical <a href="(https://arxiv.org/abs/math/0611886)">paper</a> by Chatterjee, Peled, Peres, and Romik (2010) on the gravitational allocation of Poisson points, one of the inspirations behind Diala’s work.</p>
<p>Putting (important) technical issues aside, we are ready to state the main result of our paper. We prove that, for <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5Cin(-1,1)">, the <em>repelled Poisson point process</em> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5CPi_%5Cepsilon%5Cmathcal%7BP%7D%20=%20%5C%7B%20x+%5Cepsilon%20F_%7B%5Cmathcal%7BP%7D%7D(x),%20%5Cquad%20x%5Cin%5Cmathcal%7BP%7D%20%5C%7D%0A"> is well-defined, and has on average <img src="https://latex.codecogs.com/png.latex?N"> points in <img src="https://latex.codecogs.com/png.latex?S">. Moreover, <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Ccheck%7BI%7D(f)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bx%5Cin%20S%5Ccap%20%5CPi_%5Cepsilon%5Cmathcal%7BP%7D%7D%20f(x)%0A"> is an unbiased estimator of <img src="https://latex.codecogs.com/png.latex?I(f)">. Finally, if <img src="https://latex.codecogs.com/png.latex?f"> is <img src="https://latex.codecogs.com/png.latex?C%5E2">, for <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0"> small enough, the variance of <img src="https://latex.codecogs.com/png.latex?%5Ccheck%7BI%7D(f)"> is lower than that of <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BI%7D(f)">. To sum up, for any <img src="https://latex.codecogs.com/png.latex?C%5E2"> integrand, we can in principle reduce the variance of our Monte Carlo estimator by slightly repelling the quadrature nodes away from each other. This is it: by breaking lumps and filling holes in a postprocessing step, we obtain variance reduction over crude Monte Carlo. The proof is not trivial, and relies on the super-harmonicity of the potential behind the Coulomb force.</p>
<p>Let me close with two further pointers to the <a href="https://arxiv.org/abs/2308.04825">paper</a>. First, we discuss a particular value of the “step size” parameter <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> in the paper, which has an easily-implemented closed form, and reliably led to variance reduction across our experiments. Second, while our theoretical results only cover the Poisson case so far, we also show experiments on other (stationary) point processes than Poisson, which confirm that variance reduction is also achieved across point processes with varying second-order structure. In Monte Carlo terms, and being very optimistic, some sort of repulsion might become a standard postprocessing step in the future, to reduce the variance of one’s estimator, independently of the law of the nodes (Markov chain, thinned PDMP, you name it).</p>



 ]]></description>
  <category>Monte Carlo</category>
  <category>point processes</category>
  <guid>https://github.com/statisfaction-blog/posts/01-11-2023-repelled-point-processes/repelled_point_processes.html</guid>
  <pubDate>Tue, 31 Oct 2023 23:00:00 GMT</pubDate>
</item>
<item>
  <title>particles version 0.4: single-run variance estimates, FFBS variants, nested sampling</title>
  <dc:creator>Nicolas Chopin</dc:creator>
  <link>https://github.com/statisfaction-blog/posts/01-09-2023-particles-v0.4/index.html</link>
  <description><![CDATA[ 




<p>Version 0.4 of <a href="https://github.com/nchopin/particles">particles</a> have just been released. Here are the main changes:</p>
<section id="single-run-variance-estimation-for-waste-free-smc" class="level1">
<h1>Single-run variance estimation for waste-free SMC</h1>
<p>Waste-free SMC <a href="https://academic.oup.com/jrsssb/article/84/1/114/7056097">(Dau &amp; Chopin, 2020)</a> was already implemented in particles (since version 0.3), and even proposed by default. This is a variant of SMC samplers where you resample only <img src="https://latex.codecogs.com/png.latex?M%20%5Cll%20N"> particles, apply to each resampled particle <img src="https://latex.codecogs.com/png.latex?P-1"> MCMC steps, and then gather these <img src="https://latex.codecogs.com/png.latex?M%5Ctimes%20P"> states to form the next particle sample; see the paper if you want to know why this is a good idea (short version: this tends to perform better than standard SMC samplers, and to be more robust to the choice of the number of MCMC steps).</p>
<p>What was not yet implemented (but is, in this version) is the <strong>single-run</strong> variance estimates proposed in the same paper. Here is a simple illustration:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/statisfaction-blog/posts/01-09-2023-particles-v0.4/ibis_pima_var_post.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/statisfaction-blog/posts/01-09-2023-particles-v0.4/ibis_pima_var_logLt.png" class="img-fluid figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>Both plots were obtained from <img src="https://latex.codecogs.com/png.latex?10%5E3"> runs of waste-free IBIS (i.e.&nbsp;target at time <img src="https://latex.codecogs.com/png.latex?t"> is the posterior based on the first <img src="https://latex.codecogs.com/png.latex?t+1"> observations, <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%7Cy_%7B0:t%7D)">) applied to Bayesian logistic regression and the Pima Indians dataset. The red line is the empirical variance of the output, and, since the number of runs is large, it should be close to the true variance. The lower (resp. upper) limit of the grey area is the <img src="https://latex.codecogs.com/png.latex?5%5C%25"> (resp. <img src="https://latex.codecogs.com/png.latex?95%5C%25">) quantile of the single-run variance estimates obtained from these <img src="https://latex.codecogs.com/png.latex?10%5E3"> runs. The considered output is either the posterior mean of the intercept (top) or the log marginal likelihood (bottom).</p>
<p>We can see from these plots that these single-run estimates are quite reliable, and make it possible, in case one uses IBIS, to obtain error bars even from a single run. See the documentation of module <code>smc_samplers</code> (or the scripts in <code>papers/wastefreeSMC</code>) for more details on how you may get such estimates.</p>
</section>
<section id="new-ffbs-variants" class="level1">
<h1>New FFBS variants</h1>
<p>I have already mentioned in a previous <a href="https://statisfaction.wordpress.com/2022/11/09/new-smoothing-algorithms-in-particles/">post</a>, on the old blog, that particles now implement new FFBS algorithms (i.e.&nbsp;particle smoothing algorithms that rely on a backward step) that were proposed in <a href="https://arxiv.org/abs/2207.00976">this paper</a>. On top of that, particles now also includes a hybrid version of the Paris algorithm.</p>
</section>
<section id="nested-sampling" class="level1">
<h1>Nested sampling</h1>
<p>I was invited to <a href="https://www.ipp.mpg.de/maxent2023">this</a> nested sampling workshop in Munich, so this gave me some incentive to:</p>
<ul>
<li><p>clean up and document the “vanilla” nested sampling implementation which was in module <code>nested</code>.</p></li>
<li><p>add to the same module the NS-SMC samplers of <a href="https://arxiv.org/abs/1805.03924">Salomone et al (2018)</a> to play with them and do some numerical experiments to illustrate my talk.</p></li>
</ul>
<p>I will blog shortly about the interesting results I found (which essentially are in line with Salmone et al).</p>
</section>
<section id="other-minor-changes" class="level1">
<h1>Other minor changes</h1>
<p>Several distributions and a dataset (Liver) were added, see the <a href="https://github.com/nchopin/particles/releases/tag/v0.4">change log</a>.</p>
</section>
<section id="logo" class="level1">
<h1>Logo</h1>
<p>I’ve added a <a href="https://github.com/nchopin/particles/blob/master/logo.png">logo</a>. It’s… not great, if anyone has suggestions on how to design a better log, I am all ears.</p>
</section>
<section id="whats-next" class="level1">
<h1>What’s next?</h1>
<p>I guess what’s still missing from the package are stuff like:</p>
<ul>
<li><p>the ensemble Kalman filter, which would be reasonably easy to add, and would be useful in various problems;</p></li>
<li><p>advanced methods to design better proposals, such as controlled SMC <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-5/Controlled-sequential-Monte-Carlo/10.1214/19-AOS1914.short">(Heng et al, 2020)</a> or the iterated auxiliary particle filter <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1222291">(Guarniero et al, 2017)</a>.</p></li>
</ul>
<p>If you have other ideas, let me know.</p>
</section>
<section id="feedback" class="level1">
<h1>Feedback</h1>
<p>I have not yet looked into how to enable comments on a quarto blog. You can comment by replying to this <a href="https://mathstodon.xyz/@nchopin/111018449931345157">post</a> on Mastodon, or to the same post on LinkedIn (coming soon); or you can raise an issue on github or send me an e-mail, of course.</p>


</section>

 ]]></description>
  <category>news</category>
  <category>particles</category>
  <category>SMC</category>
  <guid>https://github.com/statisfaction-blog/posts/01-09-2023-particles-v0.4/index.html</guid>
  <pubDate>Thu, 31 Aug 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Better than Monte Carlo (this post is not about QMC)</title>
  <dc:creator>Nicolas Chopin</dc:creator>
  <link>https://github.com/statisfaction-blog/posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html</link>
  <description><![CDATA[ 




<p>(This is repost from this December 2022 <a href="https://statisfaction.wordpress.com/2022/12/22/how-to-beat-monte-carlo-no-qmc/">post</a> on the old website, but since math support is so poor on Wordpress, I’d rather have this post published here.)</p>
<p>Say I want to approximate the integral <img src="https://latex.codecogs.com/png.latex?I(f)%20:=%20%5Cint_%7B%5B0,%201%5D%5Es%7D%20f(u)%20du"> based on <img src="https://latex.codecogs.com/png.latex?n"> evaluations of function <img src="https://latex.codecogs.com/png.latex?f">. I could use plain old Monte Carlo: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BI%7D(f)%20=%20%5Cfrac%201%20n%20%5Csum_%7Bi=1%7D%5En%20f(U_i),%5Cquad%20U_i%20%5Csim%20%5Cmathrm%7BU%7D(%5B0,%0A1%5D%5Es)."> whose RMSE (root mean square error) is <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1/2%7D)">.</p>
<p>Can I do better? That is, can I design an alternative estimator/algorithm, which performs <img src="https://latex.codecogs.com/png.latex?n"> evaluations and returns a random output, such that its RMSE converge quicker?</p>
<p>Surprisingly, the answer to this question has been known for a long time. If I am ready to focus on functions <img src="https://latex.codecogs.com/png.latex?f%5Cin%5Cmathcal%7BC%7D%5Er(%5B0,%201%5D%5Es)">, Bakhvalov (1959) showed that the best rate I can hope for is <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1/2-r/s%7D)."> That is, there exist algorithms that achieve this rate, and algorithms achieving a better rate simply do not exist.</p>
<p>Ok, but how can I actually design such an algorithm? The proof of Bakhvalov contains a very simple recipe. Say I am able to construct a good approximation <img src="https://latex.codecogs.com/png.latex?f_n"> of <img src="https://latex.codecogs.com/png.latex?f">, based on <img src="https://latex.codecogs.com/png.latex?n"> evaluations; assume the approximation error is <img src="https://latex.codecogs.com/png.latex?%5C%7Cf-f_n%5C%7C_%5Cinfty%20=%20O(n%5E%7B-%5Calpha%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Calpha%3E0">. Then I could compute the following estimator, based on a second batch of <img src="https://latex.codecogs.com/png.latex?n"> evaluations: <img src="https://latex.codecogs.com/png.latex?%20%5Chat%7BI%7D(f)%0A:=%20I(f_n)%20+%20%20%5Cfrac%201%20n%20%5Csum_%7Bi=1%7D%5En%20(f-f_n)(U_i),%5Cquad%20U_i%20%5Csim%0A%5Cmathrm%7BUniform%7D(%5B0,%201%5D%5Es)."> and it is easy to check that this new estimator is unbiased, that its variance is <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1-2%5Calpha%7D)">, and therefore its RMSE is <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1/2-%5Calpha%7D)">. (It is based on <img src="https://latex.codecogs.com/png.latex?2n"> evaluations.)</p>
<p>So there is strong relation between Bakhvalov results and function approximation. In fact, the best rate you can achieve for the latter is <img src="https://latex.codecogs.com/png.latex?%5Calpha=r/s">, which explain the rate above for stochastic quadrature. You can see now why I gave this title to this post. QMC is about using points that are better than random points. But here I’m using IID points, and the improved rate comes from the fact I use a better approximation of <img src="https://latex.codecogs.com/png.latex?f">.</p>
<p>Here is a simple example of a good function approximation. Take <img src="https://latex.codecogs.com/png.latex?s=1">, and <img src="https://latex.codecogs.com/png.latex?%0Af_n(u)%20=%20%5Csum_%7Bi=1%7D%5En%20f(%20%5Cfrac%7B2i-1%7D%7B2n%7D%20)%20%5Cmathbf%7B1%7D_%7B%5B(i-1)/n,%20i/n%5D%7D(u);%0A"> that is, split <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> into <img src="https://latex.codecogs.com/png.latex?n"> intervals <img src="https://latex.codecogs.com/png.latex?%5B(i-1)/n,%20i/n%5D">, and approximate <img src="https://latex.codecogs.com/png.latex?f"> inside a given interval by its value at the centre of the interval. You can quickly check that the approximation error is then <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1%7D)"> provided <img src="https://latex.codecogs.com/png.latex?f"> is <img src="https://latex.codecogs.com/png.latex?C%5E1">. So you get a simple recipe to get the optimal rate for <img src="https://latex.codecogs.com/png.latex?s=1"> and <img src="https://latex.codecogs.com/png.latex?r=1">.</p>
<p>Is it possible to generalise this type of construction to any <img src="https://latex.codecogs.com/png.latex?r"> and any <img src="https://latex.codecogs.com/png.latex?s">? The answer is in our recent paper with Mathieu Gerber, which you can find <a href="https://arxiv.org/abs/2210.01554">here</a>. You may also want to read <a href="https://arxiv.org/abs/1409.6714">Novak (2016)</a>, which is a very good entry on stochastic quadrature, and in particular gives a nice overview of Bakhvalov’s and related results.</p>



 ]]></description>
  <category>Monte Carlo</category>
  <category>QMC</category>
  <category>rates</category>
  <guid>https://github.com/statisfaction-blog/posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html</guid>
  <pubDate>Fri, 18 Aug 2023 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Welcome to the new, quarto-based version of Statisfaction</title>
  <dc:creator>Nicolas Chopin</dc:creator>
  <link>https://github.com/statisfaction-blog/posts/welcome/index.html</link>
  <description><![CDATA[ 




<p>Hey! We have just moved this blog from Wordpress to github. The old version is still available <a href="https://statisfaction.wordpress.com/">here</a>. The new version is based on <a href="https://quarto.org/">quarto</a>, which will make it much easier to write mathematics, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%7Cx)%20%5Cpropto%20%5Cpi(%5Ctheta)%20L(x%7C%5Ctheta)">, and code, e.g.&nbsp;</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fact(n):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.prod(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span></code></pre></div>



 ]]></description>
  <category>news</category>
  <guid>https://github.com/statisfaction-blog/posts/welcome/index.html</guid>
  <pubDate>Fri, 18 Aug 2023 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
