[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog initially started, a long time ago, by PhD students at ENSAE interested in Bayesian computation and related topics. Used to be hosted on Wordpress here, but now lives a happier life on github."
  },
  {
    "objectID": "posts/01-11-2023-repelled-point-processes/repelled_point_processes.html",
    "href": "posts/01-11-2023-repelled-point-processes/repelled_point_processes.html",
    "title": "Coulomb rhymes with variance reduction",
    "section": "",
    "text": "… Well, it does rhyme if you read the title aloud with a French accent, hon hon hon.\nTo paraphrase Nicolas’s previous post, say I want to approximate the integral \\[\n    I(f) := \\int_{S} f(u) du,\n\\] where \\(S\\) is a compact set of \\(\\mathbb{R}^d\\). I could use plain old Monte Carlo with \\(N\\) nodes, \\[\n    \\hat{I}(f) = \\frac 1 N \\sum_{i=1}^N f(U_i),   \\quad U_i \\sim \\mathrm{U}(S).\n\\tag{1}\\] Intuitively, an i.i.d. uniform sample of quadrature nodes \\(U_1, \\dots, U_N\\) will however leave “holes”; see Figure 1 (a). In words, given a realization of the nodes, it is possible to insert a few large balls in \\(S\\) that do not contain any \\(U_i\\). These holes may make us miss some large variations of \\(f\\). Part of the variance of the Monte Carlo estimator in Equation 1 could intuitively be removed if we managed to fill these holes, using some of the nodes that got lumped together by chance.\nMany sampling algorithms, such as randomized quasi-Monte Carlo, impose similar space-filling constraints, yielding a random sample with guarantees of “well-spreadedness”. In the paper I describe in this post, Diala Hawat and her two advisors (Raphaël Lachièze-Rey and myself) obtained variance reduction by explicitly trying to fill the holes left by a realization of \\(U_1, \\dots, U_N\\). In the remainder of the post, I will describe Diala’s main theoretical result.\n\n\n\n\n\n\n\n(a) A Poisson sample\n\n\n\n\n\n\n\n(b) The same sample after repulsion\n\n\n\n\nFigure 1: Note how the repelled sample has fewer visible “holes” and “lumps”. The details of how we implemented the repulsion are interesting in themselves, and can be found in the paper and the associated code.\n\n\nThe basic intuition is to imagine the quadrature nodes \\(U_1, \\dots, U_N\\) as electrons. In physics, electrons (like all charged particles) are subject to the Coulomb force. The Coulomb force exerted by one electron onto another points away from the first electron, with a magnitude that is inversely proportional to the \\(d-1\\)th power of the Euclidean distance between the two. As a result, electrons tend to repel each other, and electrons close to you will push you away harder than electrons at the other side of the support of \\(f\\). This is the behaviour that we would like to emulate, so that our quadrature nodes avoid lumping together and rather go and fill holes where no particle causes any repulsion.\nIf we solved the differential equation implementing Coulomb’s repulsion on our \\(N\\) i.i.d. nodes, however, the points would rapidly leave the support of \\(f\\) and “go to infinity”, to make sure that the pairwise distances between nodes are as large as possible. One way to avoid this undesired behaviour is to consider an “infinite” uniform Monte Carlo sample in \\(\\mathbb{R}^d\\), so that, wherever an electron looks, there are an infinite number of electrons preventing it from escaping. To make the situation comparable with our initial \\(N\\)-point estimator in Equation 1, we also require that there are roughly \\(N\\) points inside the region \\(S\\) where we integrate \\(f\\). Formally, we consider a homogeneous Poisson point process \\(\\mathcal{P}\\) of intensity \\(\\rho = N/V\\) in \\(\\mathbb{R}^d\\), where \\(V\\) is the volume of \\(S\\). Consider the modified Monte Carlo estimator \\[\n    \\tilde{I}(f) = \\frac{1}{N} \\sum_{x\\in S\\cap\\mathcal{P}} f(x).\n\\] This estimator is very similar to the \\(N\\)-point crude Monte Carlo estimator \\(\\hat{I}(f)\\), except the number of evaluations of \\(f\\) in the sum is now Poisson-distributed, with mean and variance \\(N\\). What we have gained is that we can now intuitively apply the Coulomb force to the points of \\(\\mathcal{P}\\), and hope that both before and after repulsion, about \\(N\\) points remain in our integration domain \\(S\\). Proving this remains technically thorny, however. For starters, for \\(x\\) in \\(\\mathbb{R}^d\\), the series defining the Coulomb force exerted on \\(x\\) by a collection \\(C\\) of points in \\(\\mathbb{R}^d\\), namely \\[\n    F_C(x) = \\sum_{y\\in C, y\\neq x} \\frac{x-y}{\\Vert x-y\\Vert^{d}},\n\\] is not absolutely convergent, so that the order of summation matters. However, it was observed as early as 1943 that, if you sum by increasing distance to the reference point \\(x\\), and \\(C=\\mathcal{P}\\) is a homogeneous Poisson point process, then the (random) series \\(F_\\mathcal{P}(x)\\) converges almost surely. Interested readers are referred to a classical paper by Chatterjee, Peled, Peres, and Romik (2010) on the gravitational allocation of Poisson points, one of the inspirations behind Diala’s work.\nPutting (important) technical issues aside, we are ready to state the main result of our paper. We prove that, for \\(\\epsilon\\in(-1,1)\\), the repelled Poisson point process \\[\n    \\Pi_\\epsilon\\mathcal{P} = \\{ x+\\epsilon F_{\\mathcal{P}}(x), \\quad x\\in\\mathcal{P} \\}\n\\] is well-defined, and has on average \\(N\\) points in \\(S\\). Moreover, \\[\n    \\check{I}(f) = \\frac{1}{N} \\sum_{x\\in S\\cap \\Pi_\\epsilon\\mathcal{P}} f(x)\n\\] is an unbiased estimator of \\(I(f)\\). Finally, if \\(f\\) is \\(C^2\\), for \\(\\epsilon&gt;0\\) small enough, the variance of \\(\\check{I}(f)\\) is lower than that of \\(\\tilde{I}(f)\\). To sum up, for any \\(C^2\\) integrand, we can in principle reduce the variance of our Monte Carlo estimator by slightly repelling the quadrature nodes away from each other. This is it: by breaking lumps and filling holes in a postprocessing step, we obtain variance reduction over crude Monte Carlo. The proof is not trivial, and relies on the super-harmonicity of the potential behind the Coulomb force.\nLet me close with two further pointers to the paper. First, we discuss a particular value of the “step size” parameter \\(\\epsilon\\) in the paper, which has an easily-implemented closed form, and reliably led to variance reduction across our experiments. Second, while our theoretical results only cover the Poisson case so far, we also show experiments on other (stationary) point processes than Poisson, which confirm that variance reduction is also achieved across point processes with varying second-order structure. In Monte Carlo terms, and being very optimistic, some sort of repulsion might become a standard postprocessing step in the future, to reduce the variance of one’s estimator, independently of the law of the nodes (Markov chain, thinned PDMP, you name it)."
  },
  {
    "objectID": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html",
    "href": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html",
    "title": "Quantum workers in Bernoulli factories",
    "section": "",
    "text": "TL;DR: A quantum computer lets you provably build more general Bernoulli factories than your laptop.\nI have grown an interest for quantum computing, both for fun and because it naturally applies to sampling my favourite distribution, determinantal point processes. One of the natural (and still quite open) big questions in quantum computing is, for a given computational task such as solving a linear system, whether having access to a quantum computer gives you any advantage over using your laptop in the smartest way possible. Maybe the quantum computer lets you solve part of your problem faster, or maybe it allows you to solve a more general class of problems. Dale, Jennings, and Rudolph (2015) prove a quantum advantage of the latter kind, for a task that appeals to a computational statistician: a quantum computer gives you access to strictly more Bernoulli factories than your laptop does. In this post, I discuss one of their examples."
  },
  {
    "objectID": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#bernoulli-factories",
    "href": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#bernoulli-factories",
    "title": "Quantum workers in Bernoulli factories",
    "section": "Bernoulli factories",
    "text": "Bernoulli factories\nFirst, I need to define what a Bernoulli factory is. Loosely speaking, a Bernoulli factory is an algorithm that, when fed with i.i.d draws from a Bernoulli random variable \\(B(p)\\) with unknown parameter \\(p\\), outputs a stream of independent Bernoullis with parameter \\(f(p)\\). The algorithm does not have access to the value of \\(p\\), and needs to work for as large a range of values of \\(p\\) as possible. For instance, a trick attributed to von Neumann gives you a Bernoulli factory for the constant function \\(f\\equiv 1/2\\), can you guess how? If you have never seen this trick, take a break and think about it. Here is a hint: try to pair Bernoulli draws and define two events of equal probability.\nThe problem of determining what Bernoulli factories can be constructed on a classical (as opposed to quantum) computer has been answered by Keane and O’Brien (1994). Essentially, it is necessary and sufficient that \\((i)\\) \\(f\\) be continuous on its domain \\(\\mathcal{P}\\), and that \\((ii)\\) either \\(f\\) is constant or there exists an integer \\(n\\) such that, for all \\(p\\in\\mathcal{P}\\), \\[\n    \\min[ f(p), 1-f(p)] \\geq \\min [ p^n, (1-p)^n ].\n\\] In particular, a non-constant \\(f\\) should not take the values \\(0\\) or \\(1\\) in \\((0,1)\\), and cannot approach these extreme values too fast. In particular, the doubling function \\(f_\\mathrm{double}:p\\mapsto 2p\\) defined on \\([0,1/2]\\) does not correspond to a Bernoulli factory, while its restriction to \\([0,1/2-\\epsilon]\\) does, for any \\(\\epsilon&gt;0\\). Another simple example is \\(f_\\mathrm{quadratic}:p\\mapsto 4p(1-p)\\) defined on \\([0,1]\\), which does not correspond to a Bernoulli factory. We now show that \\(h\\) can be realized simply on a quantum computer."
  },
  {
    "objectID": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#quantum-computers-and-quantum-coins",
    "href": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#quantum-computers-and-quantum-coins",
    "title": "Quantum workers in Bernoulli factories",
    "section": "Quantum computers and quantum coins",
    "text": "Quantum computers and quantum coins\nNow buckle up, because I need to define a mathematical model for a quantum computer. This model only requires basic algebra, albeit with strange notation. Let \\(N\\) be a positive integer, and \\[\n    \\mathbb{H} = (\\mathbb{C}^2)^{\\otimes N} = \\mathbb{C}^2\\otimes \\dots \\otimes \\mathbb{C}^2,\n\\] where the tensor product is taken \\(N\\) times. An \\(N\\)-qubit quantum computer is a machine that, when fed with\n\na linear operator \\(\\rho\\) on the Hilbert space \\(\\mathbb{H}\\), with trace norm \\(1\\) (the state),\na Hermitian operator \\(A\\) on \\(\\mathbb{C}^2\\otimes \\mathbb{C}^2\\) (the observable),\n\noutputs a draw from the random variable \\(X_{A,\\rho}\\), with support the spectrum of \\(A\\), defined by \\[\n    \\mathbb{E} g(X_{A,\\rho}) = \\mathrm{Tr}(\\rho g(A)), \\quad g:\\mathbb{H}\\rightarrow \\mathbb{R}_+.\n\\tag{1}\\] Here \\(g(A)\\) is the operator that has the same eigenvectors as \\(A\\), but where each eigenvalue \\(\\lambda\\) is replaced by \\(g(\\lambda)\\). We give two examples.\nThe quantum coin. Consider a one-qubit computer, i.e. \\(N=1\\). Then \\(\\mathbb{H} = \\mathbb{C}^2\\) has dimension \\(2\\), and we fix an orthonormal basis, which we denote by \\((\\ket{0}, \\ket{1})\\). The strange notation \\(\\ket{\\cdot}\\) is inherited from physics, and is very practical in computations, as you will see. The basic idea is that a vector in \\(\\mathbb{H}\\) is written \\(\\ket{v}\\) (a ket), and the linear form associated to \\(\\ket{v}\\in\\mathbb{H}\\) by Riesz’s theorem is denoted by \\(\\bra{v}\\) (a bra). If we denote by \\(\\braket{\\cdot\\vert\\cdot}\\) (a bra-ket, or bracket, hence the name) the inner product in \\(\\mathbb{H}\\), then by definition \\[\n    \\bra{v}: \\ket{u} \\mapsto \\braket{v\\vert u},\n\\] so that the notation for vectors and linear forms is consistent with the Hilbert structure. Now, remember we have fixed a basis \\((\\ket{0}, \\ket{1})\\) of \\(\\mathbb{H}\\); for \\(p\\in[0,1]\\), we define \\[\n    \\ket{p} = \\sqrt{p} \\ket{0} + \\sqrt{1-p}\\ket{1}.\n\\] This definition is consistent with earlier notation, as \\(\\ket{p} = \\ket{0}\\) when \\(p=0\\), for instance. Now, we define a quantum coin as the state \\(\\rho_{\\mathrm{qc}} = \\ket{p}\\bra{p}\\). It is the projection onto \\(\\mathbb{C}\\ket{p}\\), and in particular it is a linear operator of trace \\(1\\), and hence a valid state. As observable, we take the projection \\(\\ket{1}\\bra{1}\\) onto the second vector of the basis. What random variable \\(X_{\\ket{1}\\bra{1}, \\rho_{\\mathrm{qc}}\\) does this state-observable pair define in Equation 1? Well, the spectrum of the observable is \\(\\{0,1\\}\\), so we have defined a Bernoulli random variable, and the probability that it is equal to \\(1\\) is \\[\n    \\mathrm{Tr}\\left[ \\ket{1}\\bra{1} \\ket{p}\\bra{p}  \\right] = \\vert \\bra{1}\\ket{p}\\vert^2 = p.\n\\] by cyclicity of the trace. All of this to define a \\(B(p)\\) variable! Things get more puzzling when you try to create two dependent Bernoulli variables.\nTwo quantum coins. Consider now a computer with two qubits, so that the Hilbert space is \\(\\mathbb{H}=\\mathbb{C}^2\\otimes\\mathbb{C}^2\\). From our orthonormal basis \\((\\ket{0}, \\ket{1})\\) of \\(\\mathbb{C}^2\\), we can build an orthonormal basis \\((\\ket{i}\\otimes\\ket{j}, i,j\\in\\{0,1\\})\\) of \\(\\mathbb{H}\\). To keep expressions short, it is customary to write \\(\\ket{i}\\otimes\\ket{j}\\) as \\(\\ket{ij}\\). To define a pair of quantum coins, we now consider the tensor product of two quantum coins, \\[\n    \\ket{p}\\otimes \\ket{p} = p \\ket{00} + \\sqrt{p(1-p)}\\ket{01} + \\sqrt{p(1-p)}\\ket{10}+ p \\ket{11}.\n\\] The corresponding state \\(\\rho_{2\\mathrm{qc}} = (\\ket{p}\\otimes \\ket{p})(\\bra{p}\\otimes \\bra{p})\\) is a sum of sixteen terms, which we define as two quantum coins. Now consider for your observable an operator \\(B\\) with four distinct eigenvalues, say \\(\\lambda_{ij} \\in\\mathbb{C}\\) for \\(i, j\\in\\{0,1\\}\\), each corresponding to eigenvector \\(\\ket{ij}\\). The random variable \\(X = X_{B, \\rho_{2\\mathrm{qc}}\\), associated through Equation 1 to two quantum coins and our newly defined observable, has support \\(\\{\\lambda_{00}, \\lambda_{01}, \\lambda_{10}, \\lambda_{11}\\}\\), and \\[\n    \\mathbb{P}(X = \\lambda_{ij}) = \\mathrm{Tr}\\left[(\\ket{p}\\otimes \\ket{p})(\\bra{p}\\otimes \\bra{p}) \\ket{ij}\\bra{ij}\\right] = p^{1-i}(1-p)^i \\times p^{1-j}(1-p)^j,\n\\] again by cyclicity of the trace and then carefully distributing our multiplication. Otherly put, the indices of \\(X\\) are a pair of independent Bernoullis with equal paramater \\(p\\). Again, this might feel like a lot of algebraic pain for no gain. But wait for it. What if we had taken the same state, but with another observable? Say the observable with four distinct eigenvalues \\(\\lambda_{\\phi^+}, \\lambda_{\\phi-}, \\lambda_{\\psi+}, \\lambda_{\\psi-}\\in \\mathbb{C}\\), and corresponding eigenvectors \\[\n    \\ket{\\phi^{\\pm}} = \\frac{\\ket{00}\\pm\\ket{11}}{\\sqrt{2}}, \\quad\n    \\ket{\\psi^{\\pm}} = \\frac{\\ket{01}\\pm\\ket{10}}{\\sqrt{2}}.\n\\] Then, the random variable defined by Born’s rule in Equation 1 has support \\(\\{\\lambda_{\\phi^+}, \\lambda_{\\phi-}, \\lambda_{\\psi+}, \\lambda_{\\psi-}\\}\\), and is given by \\[\n    \\mathbb{P}(X_{C, \\rho_{2\\mathrm{qc}}} = \\phi^+) = \\mathrm{Tr\\left[ \\rho_{2\\mathrm{qc}} \\ket{\\phi^+}\\bra{\\ket{\\phi^+}} \\right]} =\n\\]"
  },
  {
    "objectID": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#quantum-bernoulli-factories",
    "href": "posts/TBC-quantum-bernoulli-factories/quantum-bernoulli-factories.html#quantum-bernoulli-factories",
    "title": "Quantum workers in Bernoulli factories",
    "section": "Quantum Bernoulli factories",
    "text": "Quantum Bernoulli factories\nWe first need some equivalent of a Bernoulli random variable. Consider an orthonormal basis \\((e_0, e_1)\\) of \\(\\mathbb{C}^2\\). A natural candidate is the orthogonal projector \\(\\Pi_p:\\mathbb{C}^2\\rightarrow \\mathbb{C}^2\\) onto \\[\n    \\sqrt{1-p}e_0 + \\sqrt{p}e_1\n\\] Indeed, \\(\\Pi_p\\) together with the observable \\(\\Pi_0 + \\Pi_1\\) onto \\(e_0\\) as observable define through Equation 1 a binary random variable \\(X\\) such that \\[\n    \\mathbb P (X=1) = \\mathrm{Tr}[ \\Pi_p \\Pi_1 ] = p = 1 - \\mathbb P (X=0).\n\\]\nA quantum Bernoulli factory will be defined as an algorithm that takes \\(f\\) and its domain as input, and outputs a number of qubits \\(N\\), a state-observable pair \\((\\rho, A)\\) on \\((\\mathbb{C}^2)^{\\otiems N}\\), and a function \\(h\\) such that \\(h(X_{A,\\rho})\\sim \\mathrm{Ber}(f(p))\\).\nso that \\((e_i\\otimes e_j)_{0\\leq i,j, \\leq 1}\\) gives a basis of \\(\\mathbb{H}\\). Actually, I will need to independent Bernoulli draws to realize \\(f_\\mathrm{quadratic}\\), so I directly define the equivalent of two independent Bernoulli draws. This is the state \\(v_p\\in\\mathbb{C^2}\\) defined by $$\n$$ It does represent our pair of Bernoulli draws in the following sense. If I feed my computer if you take your state \\(v_p\\) and the observable"
  },
  {
    "objectID": "posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html",
    "href": "posts/22-08-2023-monte-carlo-rates/monte_carlo_rates.html",
    "title": "Better than Monte Carlo (this post is not about QMC)",
    "section": "",
    "text": "(This is repost from this December 2022 post on the old website, but since math support is so poor on Wordpress, I’d rather have this post published here.)\nSay I want to approximate the integral \\[I(f) := \\int_{[0, 1]^s} f(u) du\\] based on \\(n\\) evaluations of function \\(f\\). I could use plain old Monte Carlo: \\[\\hat{I}(f) = \\frac 1 n \\sum_{i=1}^n f(U_i),\\quad U_i \\sim \\mathrm{U}([0,\n1]^s).\\] whose RMSE (root mean square error) is \\(O(n^{-1/2})\\).\nCan I do better? That is, can I design an alternative estimator/algorithm, which performs \\(n\\) evaluations and returns a random output, such that its RMSE converge quicker?\nSurprisingly, the answer to this question has been known for a long time. If I am ready to focus on functions \\(f\\in\\mathcal{C}^r([0, 1]^s)\\), Bakhvalov (1959) showed that the best rate I can hope for is \\(O(n^{-1/2-r/s}).\\) That is, there exist algorithms that achieve this rate, and algorithms achieving a better rate simply do not exist.\nOk, but how can I actually design such an algorithm? The proof of Bakhvalov contains a very simple recipe. Say I am able to construct a good approximation \\(f_n\\) of \\(f\\), based on \\(n\\) evaluations; assume the approximation error is \\(\\|f-f_n\\|_\\infty = O(n^{-\\alpha})\\), \\(\\alpha&gt;0\\). Then I could compute the following estimator, based on a second batch of \\(n\\) evaluations: \\[ \\hat{I}(f)\n:= I(f_n) +  \\frac 1 n \\sum_{i=1}^n (f-f_n)(U_i),\\quad U_i \\sim\n\\mathrm{Uniform}([0, 1]^s).\\] and it is easy to check that this new estimator is unbiased, that its variance is \\(O(n^{-1-2\\alpha})\\), and therefore its RMSE is \\(O(n^{-1/2-\\alpha})\\). (It is based on \\(2n\\) evaluations.)\nSo there is strong relation between Bakhvalov results and function approximation. In fact, the best rate you can achieve for the latter is \\(\\alpha=r/s\\), which explain the rate above for stochastic quadrature. You can see now why I gave this title to this post. QMC is about using points that are better than random points. But here I’m using IID points, and the improved rate comes from the fact I use a better approximation of \\(f\\).\nHere is a simple example of a good function approximation. Take \\(s=1\\), and \\[\nf_n(u) = \\sum_{i=1}^n f( \\frac{2i-1}{2n} ) \\mathbf{1}_{[(i-1)/n, i/n]}(u);\n\\] that is, split \\([0, 1]\\) into \\(n\\) intervals \\([(i-1)/n, i/n]\\), and approximate \\(f\\) inside a given interval by its value at the centre of the interval. You can quickly check that the approximation error is then \\(O(n^{-1})\\) provided \\(f\\) is \\(C^1\\). So you get a simple recipe to get the optimal rate for \\(s=1\\) and \\(r=1\\).\nIs it possible to generalise this type of construction to any \\(r\\) and any \\(s\\)? The answer is in our recent paper with Mathieu Gerber, which you can find here. You may also want to read Novak (2016), which is a very good entry on stochastic quadrature, and in particular gives a nice overview of Bakhvalov’s and related results."
  },
  {
    "objectID": "posts/01-09-2023-particles-v0.4/index.html",
    "href": "posts/01-09-2023-particles-v0.4/index.html",
    "title": "particles version 0.4: single-run variance estimates, FFBS variants, nested sampling",
    "section": "",
    "text": "Version 0.4 of particles have just been released. Here are the main changes:\n\nSingle-run variance estimation for waste-free SMC\nWaste-free SMC (Dau & Chopin, 2020) was already implemented in particles (since version 0.3), and even proposed by default. This is a variant of SMC samplers where you resample only \\(M \\ll N\\) particles, apply to each resampled particle \\(P-1\\) MCMC steps, and then gather these \\(M\\times P\\) states to form the next particle sample; see the paper if you want to know why this is a good idea (short version: this tends to perform better than standard SMC samplers, and to be more robust to the choice of the number of MCMC steps).\nWhat was not yet implemented (but is, in this version) is the single-run variance estimates proposed in the same paper. Here is a simple illustration:\n\n\n\n\n\n\n\n\n\n\nBoth plots were obtained from \\(10^3\\) runs of waste-free IBIS (i.e. target at time \\(t\\) is the posterior based on the first \\(t+1\\) observations, \\(p(\\theta|y_{0:t})\\)) applied to Bayesian logistic regression and the Pima Indians dataset. The red line is the empirical variance of the output, and, since the number of runs is large, it should be close to the true variance. The lower (resp. upper) limit of the grey area is the \\(5\\%\\) (resp. \\(95\\%\\)) quantile of the single-run variance estimates obtained from these \\(10^3\\) runs. The considered output is either the posterior mean of the intercept (top) or the log marginal likelihood (bottom).\nWe can see from these plots that these single-run estimates are quite reliable, and make it possible, in case one uses IBIS, to obtain error bars even from a single run. See the documentation of module smc_samplers (or the scripts in papers/wastefreeSMC) for more details on how you may get such estimates.\n\n\nNew FFBS variants\nI have already mentioned in a previous post, on the old blog, that particles now implement new FFBS algorithms (i.e. particle smoothing algorithms that rely on a backward step) that were proposed in this paper. On top of that, particles now also includes a hybrid version of the Paris algorithm.\n\n\nNested sampling\nI was invited to this nested sampling workshop in Munich, so this gave me some incentive to:\n\nclean up and document the “vanilla” nested sampling implementation which was in module nested.\nadd to the same module the NS-SMC samplers of Salomone et al (2018) to play with them and do some numerical experiments to illustrate my talk.\n\nI will blog shortly about the interesting results I found (which essentially are in line with Salmone et al).\n\n\nOther minor changes\nSeveral distributions and a dataset (Liver) were added, see the change log.\n\n\nLogo\nI’ve added a logo. It’s… not great, if anyone has suggestions on how to design a better log, I am all ears.\n\n\nWhat’s next?\nI guess what’s still missing from the package are stuff like:\n\nthe ensemble Kalman filter, which would be reasonably easy to add, and would be useful in various problems;\nadvanced methods to design better proposals, such as controlled SMC (Heng et al, 2020) or the iterated auxiliary particle filter (Guarniero et al, 2017).\n\nIf you have other ideas, let me know.\n\n\nFeedback\nI have not yet looked into how to enable comments on a quarto blog. You can comment by replying to this post on Mastodon, or to the same post on LinkedIn (coming soon); or you can raise an issue on github or send me an e-mail, of course."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to the new, quarto-based version of Statisfaction",
    "section": "",
    "text": "Hey! We have just moved this blog from Wordpress to github. The old version is still available here. The new version is based on quarto, which will make it much easier to write mathematics, e.g. \\(\\pi(\\theta|x) \\propto \\pi(\\theta) L(x|\\theta)\\), and code, e.g. \nimport numpy as np\n\ndef fact(n):\n    return np.prod(range(1, n + 1))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statisfaction - a collaborative blog about Bayesian computation and related topics",
    "section": "",
    "text": "Quantum workers in Bernoulli factories\n\n\n\n\n\n\n\nquantum computing\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRémi Bardenet\n\n\n\n\n\n\n  \n\n\n\n\nCoulomb rhymes with variance reduction\n\n\n\n\n\n\n\nMonte Carlo\n\n\npoint processes\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRémi Bardenet\n\n\n\n\n\n\n  \n\n\n\n\nparticles version 0.4: single-run variance estimates, FFBS variants, nested sampling\n\n\n\n\n\n\n\nnews\n\n\nparticles\n\n\nSMC\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\n  \n\n\n\n\nBetter than Monte Carlo (this post is not about QMC)\n\n\n\n\n\n\n\nMonte Carlo\n\n\nQMC\n\n\nrates\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to the new, quarto-based version of Statisfaction\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nNicolas Chopin\n\n\n\n\n\n\nNo matching items"
  }
]